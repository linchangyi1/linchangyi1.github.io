<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Changyi Lin</title>

    <meta name="author" content="Changyi Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/"> -->
    <link rel="icon" type="image/png" href="images/profile/cmu.png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <!-- <td style="padding:2%;width:30%;max-width:25%">
                <img style="width:100%;max-width:100%;" alt="profile photo" src="images/profile/photo.jpg" class="hoverZoomLink">
              </td> -->
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Changyi Lin
                </p>
                <p>
                  I am a second-year PhD student at <a href="https://safeai-lab.github.io/" target="_blank">CMU Safe AI Lab</a>, advised by Prof. <a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html" target="_blank">Ding Zhao</a>.
                  I am fortunate to work closely with Dr. <a href="https://yxyang.github.io/" target="_blank">Yuxiang Yang</a> and collaborate with the robotics team of <a href="https://deepmind.google/" target="_blank">Google Deepmind</a>.
                  <br>
                  I received dual bachelor's degrees in ME and CS from Huazhong University of Science and Technology (<a href="http://english.hust.edu.cn/" target="_blank">HUST</a>).
                  After that, I spent a gap year exploring robotics research at <a href="https://hxu.rocks/" target="_blank">Tsinghua Embodied AI Lab</a>, advised by Prof. <a href="https://hxu.rocks/" target="_blank">Huazhe Xu</a>.
                  <!-- Previously, I was a research assistant at <a href="http://hxu.rocks/" target="_blank">Tsinghua Embodied AI Lab</a>, advised by Prof. <a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>.
                  I obtained my bachelor degree in ME&CS from Huazhong University of Science and Technology (<a href="http://english.hust.edu.cn/" target="_blank">HUST</a>). -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:changyil@andrew.cmu.edu">Email</a> &nbsp;|&nbsp;
                  <a href="https://scholar.google.com/citations?user=zKMxtSIAAAAJ" target="_blank">Google Scholar</a> &nbsp;|&nbsp;
                  <a href="https://github.com/linchangyi1/" target="_blank">Github</a> &nbsp;|&nbsp;
                  <a href="https://twitter.com/changyi_lin1" target="_blank">Twitter</a> &nbsp;|&nbsp;
                  <a href="https://www.linkedin.com/in/changyi-lin-401bb7287/" target="_blank">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2%;width:30%;max-width:25%">
                <img style="width:100%;max-width:100%;" alt="profile photo" src="images/profile/photo.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>

    <style>
      .pub_highlight_color {
        /* background-color: #ffffd0;   */
        background-color: #ffffd073;  
      }
    </style>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <h2>Research</h2>
          <p>
            I am passionate about robotics, with a research focus on whole-body loco-manipulation control, visual-tactile sensing, intelligent hardware design, and reinforcement learning.
            My goal is to develop robotic systems that can fully perceive, deeply understand, and safely interact with the physical world.
            <!-- Projects I led are <span class="pub_highlight_color">highlighted</span>. -->
          </p>
        </td>
      </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()" class="pub_highlight_color">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/locotouch.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">LocoTouch: Learning Dexterous Quadrupedal Transport with Tactile Sensing</span>
          <div style="height: 0.3em;"></div>
          <strong>Changyi Lin</strong>, Yuxin Ray Song, Boda Huo, Mingyang Yu, Yikai Wang, Shiqi Liu, <br> Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Yiyue Luo, and Ding Zhao
          <div style="height: 0.5em;"></div>
          <em>Preprint 2025</em>
          <br>
          <a href="https://arxiv.org/abs/2505.23175" target="_blank">paper</a> |
          <a href="https://linchangyi1.github.io/LocoTouch/" target="_blank">website</a> |
          <a href="https://www.youtube.com/watch?v=pLm9gaQ1JXo" target="_blank">video</a> |
          <a href="https://github.com/linchangyi1/LocoTouch" target="_blank">code (coming)</a> |
          <a href="https://x.com/changyi_lin1/status/1928467406873153675" target="_blank">X</a> |
          media(<a href="https://spectrum.ieee.org/video-friday-one-legged-robot" target="_blank">IEEE Spectrum</a>)
          <!-- <br>media(<a href="https://spectrum.ieee.org/video-friday-locoman" target="_blank">IEEE Spectrum</a>,
          <a href="https://techxplore.com/news/2024-04-dexterous-legged-robot-simultaneously.html?utm_source=twitter.com&utm_medium=social&utm_campaign=v2" target="_blank">TechXplore</a>,
          <a href="https://engineering.cmu.edu/news-events/news/2024/05/13-home-helper-robot.html" target="_blank">CMU Engineering News</a>) -->
          <!-- <a href="https://interestingengineering.com/innovation/locoman-qudrupedal-robot" target="_blank">Interesting Engineering</a>
          <a href="https://www.iotworldtoday.com/robotics/robot-dog-handles-objects-with-hand-like-grippers#close-modal" target="_blank">IoT World Today</a>
          <a href="https://www.hackster.io/news/locoman-lends-robots-a-hand-927b1cb84159" target="_blank">Hackster</a>
          <a href="https://www.cryptopolitan.com/ai-enhanced-four-legged-robot-locoman-human/" target="_blank">Cryptopolitan</a> -->
          <div style="height: 0.5em;"></div>
          <em>Tactile sensing unlocks a new level of contact-rich interaction for legged robots!</em><br>
          Equipped with a high-density distributed tactile sensor, our quadrupedal robot can transport everyday objects without mounting or strapping.
          The transport policy achieves zero-shot sim-to-real transfer, featuring two task-agnostic components:
          high-fidelity tactile simulation and robust, symmetric, frequency-adaptive gaits.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/human2locoman.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">Human2LocoMan: Learning Versatile Quadrupedal Manipulation <br> with Human Pretraining</span>
          <div style="height: 0.3em;"></div>
          Yaru Niu, Yunzhe Zhang, Mingyang Yu, <strong>Changyi Lin</strong>, Chenhao Li, <br> Yikai Wang, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Zhenzhen Li, <br> Jonathan Francis, Bingqing Chen, Jie Tan, and Ding Zhao
          <div style="height: 0.5em;"></div>
          <em>RSS 2025</em>
          <br>
          <a href="https://human2bots.github.io/static/pdfs/human2locoman_rss_2025.pdf" target="_blank">paper</a> |
          <a href="https://human2bots.github.io/" target="_blank">website</a> |
          <a href="https://youtu.be/ay_-z9M18p0" target="_blank">video</a> |
          <a href="https://github.com/chrisyrniu/Human2LocoMan" target="_blank">code</a> |
          <a href="https://x.com/yaru_niu/status/1937179256897011773" target="_blank">X</a>
          <div style="height: 0.5em;"></div>
          We enable LocoMan to autonomously perform various household tasks in multiple operation modes.
          We train the policy more efficiently through pretraining on human demonstrations and fine-tuning with teleoperated robot data.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/quiet_paw.png' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">QuietPaw: Learning Quadrupedal Locomotion with Versatile Noise <br> Preference Alignment</span>
          <div style="height: 0.3em;"></div>
          Yuyou Zhang, Yihang Yao, Shiqi Liu, Yaru Niu, <strong>Changyi Lin</strong>, Yuxiang Yang, <br> Wenhao Yu, Tingnan Zhang, Jie Tan, and Ding Zhao
          <div style="height: 0.5em;"></div>
          <em>Preprint 2025</em>
          <br>
          <a href="https://arxiv.org/pdf/2503.05035" target="_blank">paper</a>
          <div style="height: 0.5em;"></div>
          By incorporating conditional policy learning with noise constraints,
          we enable quadrupedal robots to adaptively adjust their locomotion behaviors based on diverse noise preferences using a single policy.
        </td>
      </tr>


      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/dtactive.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">DTactive: A Vision-Based Tactile Sensor with Active Surface </span>
          <div style="height: 0.3em;"></div>
          Jikai Xu*, Lei Wu*, <strong>Changyi Lin</strong>, Ding Zhao, and Huazhe Xu
          <div style="height: 0.5em;"></div>
          <em>Preprint 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2410.08337" target="_blank">paper</a> |
          <a href="https://ieqefcr.github.io/DTactive/" target="_blank">website</a> |
          <a href="https://drive.google.com/file/d/1QzzIjNFbDpvQvcflzC-6dMTOlpp0q9Jp/view" target="_blank">hardware</a> |
          <a href="https://x.com/ieqefcr/status/1845663779012723115" target="_blank">X</a>
          <div style="height: 0.5em;"></div>
          We extend 9DTact with an active surface, enabling efficient continuous in-hand rotation with precise angular pose estimation across diverse objects.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/continuous_jumping.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">Agile Continuous Jumping in Discontinuous Terrains</span>
          <div style="height: 0.3em;"></div>
          Yuxiang Yang, Guanya Shi, <strong>Changyi Lin</strong>, Xiangyun Meng, Rosario Scalise, Mateo<br> Guaman Castro, Wenhao Yu, Tingnan Zhang, Ding Zhao, Jie Tan, Byron Boots
          <div style="height: 0.5em;"></div>
          <em>ICRA 2025</em>
          <br>
          <a href="https://arxiv.org/pdf/2409.10923" target="_blank">paper</a> |
          <a href="https://yxyang.github.io/jumping_cod/" target="_blank">website</a> |
          <a href="https://youtu.be/Lz_wJckllCw?si=wEvmCiXUESJolK6a" target="_blank">video</a> |
          <a href="https://github.com/yxyang/jumping_cod" target="_blank">code</a> |
          <a href="https://x.com/yxyang1995/status/1836529291095662662" target="_blank">X</a>
          <div style="height: 0.5em;"></div>
          By augmenting the GPU-accelerated RL-QP hierarchical framework (<a href="https://yxyang.github.io/cajun/" target="_blank">CAJun</a>) with perception and precise system identification,
          we enable quadrupedal robots to perform continuous, terrain-adaptive jumping over challenging environments.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()" class="pub_highlight_color">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/locoman.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight<br> Loco-Manipulators</span>
          <div style="height: 0.3em;"></div>
          <strong>Changyi Lin</strong>, Xingyu Liu, Yuxiang Yang, Yaru Niu, Wenhao Yu, Tingnan Zhang,<br> Jie Tan, Byron Boots, and Ding Zhao
          <div style="height: 0.5em;"></div>
          <em>IROS 2024, <a href="https://spectrum.ieee.org/video-friday-locoman" target="_blank" style="color: red;">Featured Cover on IEEE Spectrum</a></em>
          <br>
          <a href="https://arxiv.org/pdf/2403.18197" target="_blank">paper</a> |
          <a href="https://linchangyi1.github.io/LocoMan/" target="_blank">website</a> |
          <a href="https://www.youtube.com/watch?v=d-GLiRPiHIE" target="_blank">video</a> |
          <a href="https://github.com/linchangyi1/LocoMan" target="_blank">hardware & code</a> |
          <a href="https://x.com/changyi_lin1/status/1773368861741486371" target="_blank">X</a> |
          <br>media(<a href="https://spectrum.ieee.org/video-friday-locoman" target="_blank">IEEE Spectrum Cover</a>,
          <a href="https://techxplore.com/news/2024-04-dexterous-legged-robot-simultaneously.html?utm_source=twitter.com&utm_medium=social&utm_campaign=v2" target="_blank">TechXplore</a>,
          <a href="https://engineering.cmu.edu/news-events/news/2024/05/13-home-helper-robot.html" target="_blank">CMU Engineering News</a>)
          <!-- <a href="https://interestingengineering.com/innovation/locoman-qudrupedal-robot" target="_blank">Interesting Engineering</a>
          <a href="https://www.iotworldtoday.com/robotics/robot-dog-handles-objects-with-hand-like-grippers#close-modal" target="_blank">IoT World Today</a>
          <a href="https://www.hackster.io/news/locoman-lends-robots-a-hand-927b1cb84159" target="_blank">Hackster</a>
          <a href="https://www.cryptopolitan.com/ai-enhanced-four-legged-robot-locoman-human/" target="_blank">Cryptopolitan</a> -->
          <div style="height: 0.5em;"></div>
          <em>Limbs are not just for walking, they can be arms too!</em><br>
          We reimagine quadrupedal limbs as 6-DoF arms by equipping them with custom-designed manipulators that mimic huma wrists.
          Powered by a unified whole-body controller, LocoMan can perform a wide range of dexterous manipulation tasks.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/arraybot.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch</span>
          <div style="height: 0.3em;"></div>
          Zhengrong Xue*, Han Zhang*, Jingwen Cheng, Zhengmao He, Yuanchen Ju,<br> <strong>Changyi Lin</strong>, Gu Zhang, and Huazhe Xu
          <div style="height: 0.5em;"></div>
          <em>ICRA 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2306.16857" target="_blank">paper</a> |
          <a href="https://steven-xzr.github.io/ArrayBot/" target="_blank">website</a> |
          <a href="https://www.youtube.com/watch?v=vhhCsG38Nvw" target="_blank">video</a> |
          <a href="https://github.com/Steven-xzr/ArrayBot" target="_blank">code</a> |
          <a href="https://x.com/ZhengrongX/status/1674779737741488129" target="_blank">X</a>
          <div style="height: 0.5em;"></div>
          ArrayBot has 16x16 vertically sliding pillars, each equipped with a tactile sensor.
          By reshaping actions in the frequency domain, we train a policy that enables ArrayBot to translate a variety of everyday objects with tactile sensing.
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()" class="pub_highlight_color">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/9dtact.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">9DTact: A Compact Vision-Based Tactile Sensor for Accurate 3D Shape Reconstruction and Generalizable 6D Force Estimation</span>
          <div style="height: 0.3em;"></div>
          <strong>Changyi Lin</strong>, Han Zhang, Jikai Xu, Lei Wu, and Huazhe Xu
          <div style="height: 0.5em;"></div>
          <em>RAL & ICRA 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2308.14277" target="_blank">paper</a> |
          <a href="https://linchangyi1.github.io/9DTact/" target="_blank">website</a> |
          <a href="https://www.youtube.com/watch?v=vhhCsG38Nvw" target="_blank">video</a> |
          <a href="https://github.com/linchangyi1/9DTact" target="_blank">hardware & code</a> |
          <a href="https://drive.google.com/file/d/1zztdk5hgPj8Bgn-vsbKCrsyaGGTXFyMy/view?usp=sharing" target="_blank">dataset</a> |
          <a href="https://x.com/changyi_lin1/status/1700896880941568429" target="_blank">X</a> |
          <a href="https://www.youtube.com/watch?v=VxMceWVz7QQ" target="_blank">tutorial-en</a> |
          <a href="https://www.bilibili.com/video/BV1nu411w7t4/" target="_blank">tutorial-cn</a>
          <div style="height: 0.5em;"></div>
          <em>9D = 3D Shape Reconstruction + 6D Force Estimation!</em><br>
          Only white light! No markers! One-shot calibration! Easy fabrication! Open source!<br>
          So many features to explore! Why not check out the <a href="https://linchangyi1.github.io/9DTact/" target="_blank">website</a>?
        </td>
      </tr>

      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()" class="pub_highlight_color">
        <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/publication/dtact.gif' width=100%>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">DTact: A Vision-Based Tactile Sensor that Measures High-Resolution 3D Geometry Directly from Darkness</span>
          <div style="height: 0.3em;"></div>
          <strong>Changyi Lin</strong>, Ziqi Lin, Shaoxiong Wang, and Huazhe Xu
          <div style="height: 0.5em;"></div>
          <em>ICRA 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2209.13916" target="_blank">paper</a> |
          <a href="https://sites.google.com/view/dtact-sensor" target="_blank">website</a> |
          <a href="https://www.youtube.com/watch?v=mUg1jTWplw0" target="_blank">video</a> |
          <a href="https://github.com/linchangyi1/DTact" target="_blank">hardware & code</a> |
          <a href="https://x.com/changyi_lin1/status/1622952652014518274" target="_blank">X</a> |
          media(<a href="https://www.technology.org/2022/10/01/dtact-a-vision-based-tactile-sensor-that-measures-high-resolution-3d-geometry-directly-from-darkness/" target="_blank">Technology</a>)
          <div style="height: 0.5em;"></div>
          <em>The deeper you press, the darker you see!</em><br>
          We leverage the reflection property of translucent silicone to reconstruct 3D contact geometry using only white light.
          We also demonstrate its robustness and feasibility for non-planar contact surfaces.
        </td>
      </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:10px;width:100%;vertical-align:middle">
        <h2>Projects</h2>
      </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
          <video  width=100% muted autoplay loop>
            <source src='images/project/beam_walking.mp4' type="video/mp4" width=100%>
          </video>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">Generalizable BeamWalking for Legged Robots with Reinforcement Learning</span>
          <br>
          <em>Course Project for Intro to Robot Learning(16-831 by Deepak Pathak)</em>
          <br>
          Techniques: Reinforcement Learning, Curriculum Learning, Computer Vision
        </td>
      </tr>
    
      <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
        <td style="padding:10px;width:25%;vertical-align:middle">
          <video  width=100% muted autoplay loop>
            <source src='images/project/robot_master.mp4' type="video/mp4" width=100%>
          </video>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <span class="papertitle">RoboMaster Robotics Competition</span>
          <br>
          <em>Engineer Robot 2019, Dart System 2020, SLAM System 2021</em>
          <br>
          Techniques: Mechanical Design, Control, SLAM, Computer Vision
        </td>
      </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:10px;width:100%;vertical-align:middle">
        <h2>Talks</h2>
        <ul>
          <li>[2024/05/01] Invited Talk at <a href="https://r-pad.github.io/" target="_blank">CMU Manipulation Seminar</a></li>
        </ul>
        <ul>
          <li>[2024/04/26] Invited Talk at <a href="https://www.andrew.cmu.edu/user/amj1/locomotion_seminar.html" target="_blank">CMU Locomotion Seminar</a></li>
        </ul>
      </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:10px;width:100%;vertical-align:middle">
        <h2>Academic Services</h2>
        <ul>
          <li>Conference Reviewer: CoRL, ICRA, IROS, Humanoids
          <li>Journal Reviewer: RAL, TMECH
        </ul>
      </td>
      </tr>
    </tbody></table>

              
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:center;">
            Template from <a href="https://github.com/jonbarron/jonbarron_website" target="_blank">Jon Barron's website</a>.
          </p>
        </td>
      </tr>
    </table>
        </td>
      </tr>
    </table>
  </body>
</html>
